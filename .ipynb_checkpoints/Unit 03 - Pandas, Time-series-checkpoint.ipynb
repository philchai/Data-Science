{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pandas Functionality, Time-Series \n",
    "\n",
    "In this unit, we'll have a deeper look into how we can view, query, and combine our data in Pandas.  These methods will be important for us when it comes to viewing and cleaning data, as well as creating new columns or even entire data sets from existing ones.<br>\n",
    "Then we'll look at how to deal with data that is indexed by timestamp, including creating custom datetime indices, resampling times, changing time-zones, and omitting certain dates or times (e.g.: cutting out weekends).  Examples of these include monthly sales numbers for a business, daily temperatures, financial instrument prices, yearly population evolution, etc.  Because this data is not static, there are extra considerations we will have to make when it comes to cleaning, as well as model validation.\n",
    "\n",
    "## Pandas\n",
    "\n",
    "We've learned some basic tools for Pandas, including dataframe creation, slicing, and top/bottom viewing.  In this lesson, we'll learn more precise tools for indexing and selecting, slicing, querying, combining, and more.  The first step, as usual, is to import the relevant packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start by creating a dataframe of random integers (from 0 to 100), 20 rows by 5 columns.  This will be test data that we can play around with.  We'll have the indices be lowercase letters, and the column names be First, Second, etc.  We will use this as our example dataframe for a lot of the initial learning we'll do in this unit.  As a note, if you run the code in this section for a second time, the numbers in ```df``` will change (because they're randomly selected), so your results will be different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(np.random.randint(0,100,size=(20, 5)), columns=['First','Second','Third','Fourth','Fifth'],\n",
    "                  index=list('abcdefghijklmnopqrst'))\n",
    "# Question: how did we shorten our work for the index?\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Information, Indexing & Slicing\n",
    "\n",
    "One of the easiest ways to get an overview of our data is to use the ```df.describe``` command, which will give us a number of statistics about our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also call its length (number of rows) with the ```len()``` function, like a list or string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also call some mathematical functions on the columns, to find the sum, mean, etc. (we will see more functions in the next unit)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Third'].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We recall we can slice rows by index ```df[ix1:ix2]```, call individual columns ```df['A']```, a group of columns, ```df[['A','C','D']]```, and check the top $n$ items with ```df.head(n)``` and bottom with ```df.tail(n)```.  <br>\n",
    "(Note:  when the output is large, we can click on the rectangle where ```Out[]:``` is contained to make it smaller)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df[['Second','Third']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['a':'i']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(This slice was by *label*, not integer index; what do you notice about the beginning/end of this slice?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more advanced selection by index and column, we use the .loc[] command.  The common format is ```df.loc[ rows, columns]```\n",
    "where the rows and columns can be sliced or given as lists.  For example, if we wanted to take our dataframe, and slice out rows 'a' to 'f' for columns 'First' and 'Fifth' only, we would write: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc['a':'f',['First','Fifth']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could slice along both; taking rows `a` through `f` and the first three columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc['a':'f','First':'Third']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we wanted the value of an individual item, there are a number of ways we can access it.  If we use the above syntax to slice along the same row for a single column (e.g.: row 'a', column 'First'), we get:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc['a':'a','First']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This visually displays the information, but the format looks a bit weird.  Let's check what the type is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(df.loc['a':'a','First'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see this outputs a Series object.  If wanted just the value (say, to assign to a variable), we can use ```.loc[row,column]```:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc['a','First']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though we have row labels, we can still access values with integer index numbering by refering to the index object of the dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df.index[0],'First']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we knew a row label, but wanted to know what its integer index is, we can use the ```.index.get_loc(label)```:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.index.get_loc('a')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can access a list of the column names with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and we can access its values as we would a normal list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns[::2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an exercise, let's consider the following dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "health_dict = { 'Height':[5.04,5.11,5.09,4.11,5.07], 'Weight':[125,160,131,98,195],'Age':[76,34,28,54,42]}\n",
    "df_ex = pd.DataFrame(health_dict,index=['Irene','Bertrand','Constance','Agatha','Wilberforce'])\n",
    "df_ex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How would we query the age and weight of the Irene, Bertrand, and Constance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How could we query the age and height of the first and third rows only?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataframe Concatenation\n",
    "\n",
    "Just like with strings, we can concatenate dataframes together to create larger dataframes.  The difference is that we have an extra dimension to consider: do we want to concatenate along the rows, or along the columns?  Let's make two small dataframes to work with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc1 = pd.DataFrame({'a':['a0','a1'],'b':['b0','b1']},index=[0,1])\n",
    "dc2 = pd.DataFrame({'a':['a2','a3'],'b':['b2','b3']},index=[2,3])\n",
    "\n",
    "dc1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To concatenate these dataframes, we use the function ```pd.concat()```, and we pass the function a *list* of dataframes that we want to concatenate (the order of the list is the order that they will be combined).  We also need to tell it the axis we want to concatenate along.  Concatenating along rows is ```axis=0```, and is the default.  Concatenating along the columns is ```axis=1```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([dc1,dc2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([dc1,dc2],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we change the order:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([dc2,dc1],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also concatenate using `.append()`.  **Note**:  like the `.concat()` method, this returns a copy, and does not modify the original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc1.append(dc2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the following two dataframes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc3 = pd.DataFrame({'a':['a0','a1'],'b':['b0','b1']},index=[0,1])\n",
    "dc4 = pd.DataFrame({'a':['a2','a3'],'b':['b2','b3']},index=[1,2])\n",
    "\n",
    "dc3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They share a common index.  What happens to the index when we concatenate these two dataframes along rows?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([dc3,dc4],axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How could we fix this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since they share a common index element, if we concatenate along columns, we have two options.  The default option we've seen is an *outer* join, where no information is lost:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([dc3,dc4],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could also perform an *inner* join, which will only keep the rows with a common index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([dc3,dc4],axis=1,join='inner')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also perform a slightly more complicated join, where we can specify which dataframe's index to use in the final version.  For example, if we `axis=1` concatenate `dc3` and `dc4`, and specify to use `dc3`'s index, we get:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([dc3,dc4],axis=1,join_axes=[dc3.index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Keys\n",
    "\n",
    "If we wanted to concatenate a number of dataframes, but also keep track of the individual pieces, we could assign *keys* to the data frames.  For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([dc1,dc2],keys=['x','y'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we assign this concatenation to a variable name, we can then pull the individual keys any time we need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "together = pd.concat([dc1,dc2],keys=['x','y'])\n",
    "together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `.loc[]` command will give us the keyed sections:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "together.loc['x']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding these keys turns the index of the dataframe into a *multiindex*, giving more than one layer of indexing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "together.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "together.loc[('x',1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This provides us with a (potentially more complicated) way to get around concatenating with repeated indices.  With our example of `[dc3,dc4]` above, we wouldn't have to do any re-indexing on the original indices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([dc3,dc4],axis=0,keys=['x','y'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This way, the two indices with `1` are clearly delineated by their respective keys.\n",
    "\n",
    "As a note, pulling the keyed sections with `.loc[]` and making adjustments to them will not result in the original dataframe changing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "together.loc['x'] + 'g'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Querying data with conditions, adjusting data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of the selection we've done up until this point has been by row or column label.  What if we wanted to find out what parts of our data set satisfy a specific condition?  There are a number of ways to do this, that yield different-looking results:\n",
    "1. Using the syntax ```df[conditions]```, Pandas will return a subset of the dataframe that matches the condition given.\n",
    "2. Using the ```.where()``` command, Pandas will return an object the same dimensions as the original, but will fill in everything that doesn't fit the criteria with a value of your choice.\n",
    "\n",
    "To begin with, let's find all rows where the values of ```'Seconds'``` are greater than 50."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['Second']>50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice that this returns all of the columns for the rows that satisfy the criterion on ```df['Second']```.  How do you think we would call *just* the data from 'Second'?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This produces a Series object.  How would we get it to return a Dataframe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can combine multiple conditions, but we have to be careful with the syntax.  We cannot use the ```and,or``` as words, but must use the symbols ```&,|```.  Also, we must wrap each condition in parentheses, so Pandas knows where to parse their beginnings and endings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[(df['Second']>50) & (df['Fourth']<20)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get error messages without parentheses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['Second']>50 & df['Fourth']<20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How would we query the values from the column `Fourth` corresponding to the rows where the values in `First` are either bigger than 75 or smaller than 20?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another option for querying is using Pandas' ```.where()``` command.  It has the syntax<br>\n",
    "``` df.where(condition, what to fill with where condition is false)```<br>\n",
    "If you do not put a \"what to fill\" value in, it will default to replacing the False values of the condition to np.NaN.  Using the same example as above, finding where ```df['Second']``` is greater than 50:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.where(df['Second']>50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we wanted to make this cleaner looking, we could tell Pandas to replace everything that doesn't satisfy the condition with a 0, or even just an empty string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.where(df['Second']>50,'')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's important to note that this does *not* change the original dataframe, which we can confirm by calling it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How would we make the change in-place?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We saw in the previous unit that we could replace a single column's data using assignment and computations.  To replace the column `Fifth` by all of the values in that column multiplied by 2, we'd write:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Fifth'] = df['Fifth']*2\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But what if we wanted much finer control over what we're replacing?  It turns out we can use the ```df.loc[]``` command to change data with more precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc['h':'h',:] = 1,2,3,4,5\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can replace more than one row at a time by passing lists of lists:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = df.copy()\n",
    "df3.loc['a':'b','First':'Third'] = [[10,20,30],[99,80,79]]\n",
    "df3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How would we change just an individual piece of data using this method, instead of a row or column?  Let's replace row ```'j'```, column ```'Third'``` with 1000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, note that we created `df3` by specifying that it should be a *copy* of `df`:  `df3 = df.copy()`.  What would have happened if we had just defined `df3 = df` and made the changes?  Let's test with a smaller example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.DataFrame(np.random.randint(0,100,size=(4, 4)),columns=list('ABCD'))\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t3 = test\n",
    "t3.loc[0,:] = [0,0,0,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do you think this will affect both dataframes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sorting, Grouping\n",
    "\n",
    "Given a set of data with comparable entries (e.g.: numerical), we can sort the data in a number of ways.  We can sort along axes using the `.sort_index()` method.  In this case, we have to specify the axis (`0` refers to rows, `1` to columns) and whether to sort ascending or not.  For example, we could sort `df` by row labels in reverse order by:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_index(axis=0, ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or we could sort by column labels in ascending order:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_index(axis=1,ascending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can sort by values by using the `.sort_values()` method.  In this case, we have to tell Pandas how to sort; we can sort by a column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values(by='First',ascending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i.e.: it reorders the row index so that the column `First` is in ascending order by values.  \n",
    "\n",
    "In cases where it makes sense, we can sort by more than one column.  For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test2 = df = pd.DataFrame({'X' : ['A', 'A', 'B','B', 'D', 'C'],'Y' : [2, 0, 4, 5, 1, 2],'Z': [0, 1, 9, 4, 2, 3]})\n",
    "test2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test2.sort_values(by=['X','Y'],ascending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does this differ if we had changed the order of `X` and `Y`?  Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test2.sort_values(by=['Y','X'],ascending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How would we produce the data in `df` where `First` is bigger than 50 or `Fifth` is smaller than 50, sorted by the column `Second`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also group data together using `groupby`; the Pandas documentation lists this as involving one or more of the following operations:\n",
    "\n",
    "* Splitting the data into groups based on some criteria\n",
    "* Applying a function to each group independently\n",
    "* Combining the results into a data structure\n",
    "\n",
    "Using the `test2` dataframe above, we could group using the column `X`, and then find the mean of the groups (statistical functionality we'll be seeing more of later!):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test2.groupby('X').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can set the `groupby` to a variable name, and call various parts of it as we need it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grouped = test2.groupby('X')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It doesn't actually do any splitting until we need to, as can be seen by calling the object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can access the groups by label using `get_group()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped.get_group('A')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped.get_group('C')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can even iterate through all of the names & groups in the object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, group in grouped:\n",
    "    print(name)\n",
    "    print(group)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also call `describe()` on the `groupby` object to get descriptions of each group:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Unit 0 review question:  why are the standard deviations of groups `C` and `D` listed as `NaN`?)\n",
    "\n",
    "What would it look like if we grouped by column `Z` instead?  Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shifting Data\n",
    "\n",
    "We can move data up and down using the ```.shift()``` command.  If we put a positive number in as the argument, it will shift the rows *down* by that amount; negative numbers will shift the rows *up*.  New rows along the indices will be filled in with np.NaN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shift(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why would we want to move our data?  Well, combined with the querying we've learned, it will allow us to compare data from one row with data from another.  This is especially useful in time-series data, where we can ask questions like \"for what time periods did the price go up?\", or \"when is the population falling?\", etc.<br>\n",
    "Let's find all of the places where the data in column ```'Fifth'``` is three more than the previous entry from `df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time-Series Data\n",
    "\n",
    "One common type of data set is time-series data, which has sequential-timestamps as its index.  In this section, we'll learn about the datetime object, with focus on the following topics:\n",
    "* Generating datetime ranges.\n",
    "* Creating / loading data sets that have datetime indices.\n",
    "* Resampling time-series data.\n",
    "* Time zones conversions.\n",
    "\n",
    "The main way to create a sequence of datetimes is to use the ```pd.date_range()``` function.  At minimum, you must specify the following arguments to create a date_range:\n",
    "1. ```start```, to say when the sequence will begin.\n",
    "2. ```end``` *or* ```periods```, providing either a specific end, or the number of periods from the ```start```.\n",
    "3. ```freq```, the frequency of the times.  The default is ```'D'``` for calendar daily, but other options include ```'M'``` for monthly, ```'Y'``` for yearly, ```'H'``` for hourly, ```'T'``` or ```'min'``` for minutely, and ```'s'``` for secondly.\n",
    "Here are two examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = pd.date_range('1/1/2014', periods=28, freq='H')\n",
    "dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates2 = pd.date_range('2015-02-05', '2015-02-14', freq='D')\n",
    "dates2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a dataframe now using a smaller version of the ```dates``` object above as its index.  We'll make it with three columns of random integers, representing hourly stock price changes, starting at 8:00:00 and going until 16:00 on June 1st 2014.  First, let's create that date range:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dates3 = pd.date_range('2014-06-01 08:00:00','2014-06-01 16:00:00',freq='H')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dft = pd.DataFrame(np.random.randint(-100,100,size=(9, 3)), columns=['Stock 1','Stock 2','Stock 3'],\n",
    "                  index=dates3)\n",
    "dft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can slice along rows using the same syntax as before, feeding the timestamps as strings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dft['2014-06-03 10:00:00':'2014-06-03 13:00:00']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dft.loc['2014-06-03 10:00:00':'2014-06-03 11:00:00',['Stock 1', 'Stock 3']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resampling Time-Series Data\n",
    "\n",
    "Resampling data is a method of scaling the time index, and obtaining some kind of insight about the data along the times scaled.  For example, with our stock data in ```dft```, we may wish to know what the mean price is over two-hour periods.  The syntax for resampling has changed recently in Pandas, and now has the following format: ```dataframe.resample(timeperiod).agg(instructions)```. <br>\n",
    "So the average price in two-hour periods of our stocks can be obtained by:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dft.resample('2H').agg('mean')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the sum of the price changes over four hour periods is given by:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dft.resample('4H').agg('sum')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the common methods of resampling time-series data happens in finance.  When given a list of all possible transaction prices (\"tick data\"), or a high-frequency set of prices (e.g.: second-data), one may want to condense the data into an overview at a longer time period.  The usual way of accomplishing this is to resample the prices to the first, maximum, minimum, and last price for each longer period (known as \"open-high-low-close\" format).  Let's produce a set of prices at the one-second level to start."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "date_seconds = pd.date_range('2017-01-01 09:30:00','2017-01-01 10:00:00',freq='S')\n",
    "df_p = pd.DataFrame(np.random.randint(540,555,size=len(date_seconds)),columns=['Price'],index=date_seconds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll follow the same resampling syntax as above to move to the one-minute level, but in the ```.arg()``` section we will provide it with a list of what we want.  Luckily, Python understands words like \"first\", \"max\", etc., in this context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_minutes= df_p.resample('1min').agg(['first','max','min','last'])\n",
    "df_minutes.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we wanted to keep this more in line with finance conventions, we could rename the columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_minutes.columns = ['open','high','low','close']\n",
    "df_minutes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we wanted to find the times where the price increased at the second level in ```df_p```, how would we accomplish this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How could we find the times where the price increased at the minute level in ```df_minutes```?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's resample our minute data data again.  We'll try to find out some statistics about the open prices, by resampling to 5 minute data and finding the mean and max."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading Time-Series Files\n",
    "\n",
    "Reading in CSVs that have time-series indices is nearly the same as what we saw in the last unit, but there is an extra step.  If we use the example file \"timeseries_1.csv\", and tell it the index column is 0, we get the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = pd.read_csv('timeseries_1.csv',index_col=0)\n",
    "ts.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, those index elements don't look like the datetime objects we've been dealing with so far, and there's a good reason:  they're not.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is clearly evidenced when we try to do our standard slicing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts['2014-02-01 08:00':'2014-02-01 08:03']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Though slicing with the index *as written* works)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts['02/01/2014 8:00':'02/01/2014 8:03']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we call the index, we see:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ```dtype``` is listed as ```object```, whereas we saw before that the ```date_range()``` function gives ```dtype='datetime64[ns]'```.  In other words, when we read the file, Pandas left the index as-is (as strings), and didn't turn them into datetime objects.  The way we fix this is that when we read in the file, we need to tell Pandas to parse anything that looks like dates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = pd.read_csv('timeseries_1.csv',index_col=0,parse_dates=True)\n",
    "ts.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That looks much better!  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ts.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now our standard slicing works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts['2014-02-01 08:00':'2014-02-01 08:03']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And since the index now contains proper datetime objects, slicing in the original format *also works*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts['02/01/2014 8:00':'02/01/2014 8:03']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time Zones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repositories of time-series data will often convert everything to Coordinated Universal Time (UTC), as a way to standardize data from different time zones.  Luckily, Pandas has a lot of excellent functionality to deal with conversions.  In fact, we can convert from one time zone to another in 2 or 3 lines of code (depending on your formatting preferences).  To begin, let's make a copy of our 9:30 to 10:00 price data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_conv = df_p\n",
    "df_conv.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step to converting is we have to specify what time zone the current data is in.  We use the ```.tz_localize()``` function for this.  Let's tell the program that our data is in UTC:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_conv.index = df_conv.index.tz_localize('UTC')\n",
    "df_conv.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we want to convert it to EST with daylight savings times accounted for.  To accomplish this, we use ```.tz_convert()``` on the index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_conv.index = df_conv.index.tz_convert('US/Eastern')\n",
    "df_conv.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now our times have been converted.  If there is no ambiguity in the time zone for the data set, you can remove the time zone markers ```-05:00``` by re-localizing the time zone to ```None```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_conv.index = df_conv.index.tz_localize(None)\n",
    "df_conv.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing Time-Series\n",
    "\n",
    "We learned about using the `.shift()` method to move data.  This can be very useful for time-series data for finding specific kinds of changes in the data.  If we look at our price data timeseries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How would we find all of the minutes that the `high` decreased compared to the previous minute?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now how would we find all of the minutes where the open was $0.1$ or more bigger than the previous minute's low?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time-series data has a few nuances compared to static data.  In particular cleaning missing data becomes more complicated (with more options), and validating models that are predictive on time-series data can't be done in a lot of the \"normal\" ways.  We will see these issues soon."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3\n",
    "\n",
    "1. Consider the first dataframe example we used in this unit, ```df = pd.DataFrame(np.random.randint(0,100,size=(20, 5)), columns=['First','Second','Third','Fourth','Fifth'],index=list('abcdefghijklmnopqrst')) ```.<br>\n",
    "We've seen how to add new columns, how to redefine columns, and how to redefine rows.  How would we add a new row, using the tools we've already learned?  Add a row with index ```'u'``` to df, with values ```0,1,-1,2,-2```. Then add a row with index ```'v'``` which is the sum of rows ```'b'``` and ```'d'```.\n",
    "2. Read in the ```timeseries_1.csv``` file.  \n",
    "   1. Find the mean of the high prices for the minutes where the volume is greater than 3000.  \n",
    "   2. Suppose we have a trade signal which tells us to buy if the close price minus the open price of the previous minute is positive, and the volume of the previous minute is greater than that of the minute before it.  How many times would we buy? \n",
    "3. Consider the following dataframe of futures contract transactions:  <br> ```pc = pd.DataFrame({'Buy_or_Sell':['B','S','B','S','S','B'],'Price':[2204.0,2275.5,2250.75,2230.0,2280.75,2282.25],'Qty':[1,2,1,2,1,3]}) ```.  \n",
    "    1. Create a new column called 'Value', which is Price * Qty, but is negative if the transaction is a buy, and positive if the transaction is a sell (why?).  \n",
    "    2. How would we determine the money we made on these transactions? (Assume the prices given are in dollars.)\n",
    "    3. Find the average (mean) price of buy transactions.\n",
    "\n",
    "4.  The file `customer_data.csv` contains some data on the business customers of a large telecommunications firm. The columns are specified as follows:\n",
    " * INDUSTRY: the customer's industry\n",
    " * EMP: the number of employees that work at the business\n",
    " * ANNUAL_SALES: the business's annual sales\n",
    " * PROVINCE: the business headquarters\n",
    " * MOBILITY: the amount the customer has spent on mobility products (cell phones, tablets, etc.) in the last 12 months\n",
    " * INTERNET: the amount the customer has spent on Internet products in the last 12 months.\n",
    " \n",
    " Import the file and answer the following questions.\n",
    "   1. The marketing department would like to target a campaign to medium sized (100-999 Employees) customers who are in either \n",
    "   2. Education or Food Services, and who currently spend less than $\\$5000$ total per year on all services. Create a boolean column called INCLUDE that indicates whether the customer on that row should be included in the campaign.\n",
    "   3. How many customers are in the campaign?\n",
    "   4. Pandas has a function called crosstab which prints cross tabulations of categorical variables. Use the crosstab function to determine how many customers in each industry and province are included in the campaign.\n",
    "   5. Create a file called customerids.csv that contains only a list of CUSTOMER_IDs to be included in the campaign.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
